{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Optimizer Lecture\n",
    "*Von Lars Heimann, Julian Sibbing and Annika Terhörst*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup\n",
    "1. Beginnen Sie damit, den ``import`` Block auszuführen, um zu sehen, ob die benötigten Pakete installiert sind. Installieren Sie fehlende Pakete, bis Sie den Block erfolgreich ausführen können.\n",
    "2. Gehen Sie durch die Sektionen. Führen Sie die folgenden Schritte für jeden Abschnitt aus\n",
    "    1. Führen Sie den `Setup`-Block aus. Dadurch werden alle notwendigen Funktionen für den Abschnitt erstellt.\n",
    "    2. Bearbeite den Block `Task`.\n",
    "    3. Visualisieren und Testen mit dem `Visualize`-Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from typing import Tuple, List, Callable, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Setup Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktionsdefinitionen\n",
    "def steep_valley_function():\n",
    "    # Funktionsterm\n",
    "    def f(x, y):\n",
    "        return 0.5 * x**2 + 2 * y**2\n",
    "\n",
    "    # Gradient der Funktion\n",
    "    def grad(x, y):\n",
    "        grad_x = x  # partielle Ableitung nach x\n",
    "        grad_y = 4 * y  # partielle Ableitung nach y\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    # Bauen der Funktion\n",
    "    x = np.linspace(-20, 20, 400)\n",
    "    y = np.linspace(-20, 20, 400)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "\n",
    "def l_shaped_valley_function():\n",
    "    # Funktionsterm\n",
    "    def f(x, y):\n",
    "        return np.where(x > y, (y + 20) ** 2 + x, (x + 20) ** 2 + y)\n",
    "\n",
    "    # Gradient der Funktion\n",
    "    def grad(x, y):\n",
    "        grad_x = np.where(x > y, 1, 2 * (x + 20))  # partielle Ableitung nach x\n",
    "        grad_y = np.where(x > y, 2 * (y + 20), 1)  # partielle Ableitung nach y\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    # Bauen der Funktion\n",
    "    x = np.linspace(-50, 20, 400)\n",
    "    y = np.linspace(-50, 20, 400)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "\n",
    "def asymmetric_convex_function():\n",
    "    # Funktionsterm\n",
    "    def f(x, y):\n",
    "        return (\n",
    "            0.3 * (x - 10) ** 2\n",
    "            + 0.2 * (y - 10) ** 2\n",
    "            + 1.1 * (x + 10) ** 2\n",
    "            + 1 * (y + 10) ** 2\n",
    "        )\n",
    "\n",
    "    # Gradient der Funktion\n",
    "    def grad(x, y):\n",
    "        grad_x = 0.6 * (x - 10) + 2.2 * (x + 10)  # partielle Ableitung nach x\n",
    "        grad_y = 0.4 * (y - 10) + 2 * (y + 10)  # partielle Ableitung nach y\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    # Bauen der Funktion\n",
    "    x = np.linspace(-20, 20, 400)\n",
    "    y = np.linspace(-20, 20, 400)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "\n",
    "def symmetric_convex_function():\n",
    "    # Funktionsterm\n",
    "    def f(x, y):\n",
    "        return x**2 + y**2\n",
    "\n",
    "    # Gradient der Funktion\n",
    "    def grad(x, y):\n",
    "        grad_x = 2 * x  # partielle Ableitung nach x\n",
    "        grad_y = 2 * y  # partielle Ableitung nach y\n",
    "        return np.array([grad_x, grad_y])\n",
    "\n",
    "    # Bauen der Funktion\n",
    "    x = np.linspace(-20, 20, 100)\n",
    "    y = np.linspace(-20, 20, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    return X, Y, Z, f, grad\n",
    "\n",
    "\n",
    "# Learning Rate Scheduling\n",
    "def power_scheduling(initial_lr, t, s, c=1):\n",
    "    return initial_lr / (1 + t / s) ** c\n",
    "\n",
    "\n",
    "def exponential_scheduling(initial_lr, t, s):\n",
    "    return initial_lr * (0.1 ** (t / s))\n",
    "\n",
    "\n",
    "def piecewise_constant_scheduling(epoch, boundaries, values):\n",
    "    for boundary, value in zip(boundaries, values):\n",
    "        if epoch < boundary:\n",
    "            return value\n",
    "    return values[-1]\n",
    "\n",
    "\n",
    "def performance_scheduling(\n",
    "    validation_error, prev_validation_error, lr, lambda_factor=0.1\n",
    "):\n",
    "    if validation_error > prev_validation_error:\n",
    "        return lr * lambda_factor\n",
    "    return lr\n",
    "\n",
    "\n",
    "def one_cycle_scheduling(t, total_steps, initial_lr, max_lr):\n",
    "    if t < total_steps / 2:\n",
    "        return initial_lr + (max_lr - initial_lr) * (2 * t / total_steps)\n",
    "    return max_lr - (max_lr - initial_lr) * (2 * (t - total_steps / 2) / total_steps)\n",
    "\n",
    "\n",
    "# Anwenden des Schedulers auf Learning Rate in Schritt t\n",
    "def apply_lr_scheduler(lr_scheduler, t, lr, scheduler_params):\n",
    "    if lr_scheduler is None or lr_scheduler == \"None\":\n",
    "        return lr\n",
    "    elif lr_scheduler == \"Power\":\n",
    "        return power_scheduling(lr, t, scheduler_params[\"Power\"])\n",
    "    elif lr_scheduler == \"Exponential\":\n",
    "        return exponential_scheduling(lr, t, 2)  # 2 als default fuer s\n",
    "    elif lr_scheduler == \"Piecewise\":\n",
    "        boundaries = list(map(int, scheduler_params[\"Piecewise Boundaries\"].split(\",\")))\n",
    "        values = list(map(float, scheduler_params[\"Piecewise Values\"].split(\",\")))\n",
    "        return piecewise_constant_scheduling(t, boundaries, values)\n",
    "    elif lr_scheduler == \"Performance\":\n",
    "        prev_validation_error = 0  # Platzhalter, da Loss nicht beruecksichtigt wird\n",
    "        return performance_scheduling(\n",
    "            0, prev_validation_error, lr, scheduler_params[\"Lambda Factor\"]\n",
    "        )\n",
    "    elif lr_scheduler == \"OneCycle\":\n",
    "        total_steps = scheduler_params[\"Total Steps\"]\n",
    "        max_lr = scheduler_params[\"Max LR\"]\n",
    "        return one_cycle_scheduling(t, total_steps, lr, max_lr)\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "\n",
    "# Standardisierte optimize-Funktion\n",
    "def optimize(\n",
    "    optimizer_type=\"Regular\",\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    lr_scheduler=None,\n",
    "    scheduler_params=None,\n",
    "    print_res=False,\n",
    ") -> Tuple[Callable[[float, float], float], List[np.ndarray], List[float], str]:\n",
    "\n",
    "    # Fuehre den gewaehlten Optimizer aus\n",
    "    if optimizer_type == \"Regular\":\n",
    "        return regular_gradient_descent(\n",
    "            function, steps, lr, cur_pos, print_res, lr_scheduler, scheduler_params\n",
    "        )\n",
    "    elif optimizer_type == \"Momentum\":\n",
    "        return momentum_gradient_descent(\n",
    "            function,\n",
    "            steps,\n",
    "            lr,\n",
    "            beta1,\n",
    "            cur_pos,\n",
    "            print_res,\n",
    "            lr_scheduler,\n",
    "            scheduler_params,\n",
    "        )\n",
    "    elif optimizer_type == \"Nesterov\":\n",
    "        return nesterov_gradient_descent(\n",
    "            function,\n",
    "            steps,\n",
    "            lr,\n",
    "            beta1,\n",
    "            cur_pos,\n",
    "            print_res,\n",
    "            lr_scheduler,\n",
    "            scheduler_params,\n",
    "        )\n",
    "    elif optimizer_type == \"AdaGrad\":\n",
    "        return adagrad_gradient_descent(\n",
    "            function, steps, lr, cur_pos, print_res, lr_scheduler, scheduler_params\n",
    "        )\n",
    "    elif optimizer_type == \"RMSProp\":\n",
    "        return rmsprop_gradient_descent(\n",
    "            function,\n",
    "            steps,\n",
    "            lr,\n",
    "            beta1,\n",
    "            cur_pos,\n",
    "            print_res,\n",
    "            lr_scheduler,\n",
    "            scheduler_params,\n",
    "        )\n",
    "    elif optimizer_type == \"Adam\":\n",
    "        return adam_gradient_descent(\n",
    "            function,\n",
    "            steps,\n",
    "            lr,\n",
    "            beta1,\n",
    "            beta2,\n",
    "            cur_pos,\n",
    "            print_res,\n",
    "            lr_scheduler,\n",
    "            scheduler_params,\n",
    "        )\n",
    "    elif optimizer_type == \"Custom\":\n",
    "        return custom_gradient_descent(\n",
    "            function,\n",
    "            steps,\n",
    "            lr,\n",
    "            beta1,\n",
    "            cur_pos,\n",
    "            print_res,\n",
    "            lr_scheduler,\n",
    "            scheduler_params,\n",
    "        )\n",
    "\n",
    "\n",
    "# Ausfuehren mehrerer Optimizer\n",
    "def optimize_multiple(\n",
    "    optimizers=[\"Regular\", \"momentum\"],\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "):\n",
    "    results = []\n",
    "    for optimizer in optimizers:\n",
    "        results.append(\n",
    "            optimize(optimizer, function, steps, lr, beta1, beta2, cur_pos, print_res)\n",
    "        )\n",
    "    return results\n",
    "\n",
    "\n",
    "# Plotting Funktion, die eine Liste von Ergebnissen nimmt und ein Plot zurueckgibt (noch nicht interaktiv)\n",
    "def plot_optimization_path(optimizer_results):\n",
    "    fig = plt.figure(figsize=(24, 8))\n",
    "    ax1 = fig.add_subplot(1, 2, 1, projection=\"3d\")  # 3D-Visualisierung\n",
    "    ax2 = fig.add_subplot(1, 2, 2)  # 2D Heatmap\n",
    "\n",
    "    # Funktion aus ersten Optimizer herleiten\n",
    "    f = optimizer_results[0][0]\n",
    "    x_range = np.linspace(-20, 20, 1000)\n",
    "    y_range = np.linspace(-20, 20, 1000)\n",
    "    X, Y = np.meshgrid(x_range, y_range)\n",
    "    Z = f(X, Y)\n",
    "\n",
    "    # Ermitteln des Minimums\n",
    "    min_z = np.min(Z)\n",
    "    min_pos = np.where(Z == min_z)\n",
    "    min_x = X[min_pos][0]\n",
    "    min_y = Y[min_pos][0]\n",
    "\n",
    "    # Plot 3D Visualisierung\n",
    "    ax1.plot_surface(X, Y, Z, alpha=0.5, cmap=\"viridis\", edgecolor=\"none\")\n",
    "    ax1.set_title(\"3D View - Optimization Paths\")\n",
    "\n",
    "    # Markiere Minimum in 3D\n",
    "    ax1.scatter(\n",
    "        min_x, min_y, min_z, marker=\"8\", color=\"black\", s=30, alpha=0.5, label=\"Minumum\"\n",
    "    )\n",
    "    # ax1.text(min_x, min_y, min_z, f'({min_x:.2f}, {min_y:.2f}, {min_z:.2f})', color='red')\n",
    "\n",
    "    # Plot 2D heatmap\n",
    "    ax2.contourf(X, Y, Z, levels=100, cmap=\"viridis\")\n",
    "    ax2.set_title(\"Top-Down View (2D) - Optimization Paths\")\n",
    "    ax2.set_xlabel(\"X\")\n",
    "    ax2.set_ylabel(\"Y\")\n",
    "\n",
    "    ax2.scatter(\n",
    "        min_x,\n",
    "        min_y,\n",
    "        marker=\"8\",\n",
    "        color=\"black\",\n",
    "        linewidths=5,\n",
    "        alpha=0.5,\n",
    "        label=\"Minumum\",\n",
    "    )\n",
    "\n",
    "    # Farbschema fuer Optimizer\n",
    "    color_map = {\n",
    "        \"Regular\": \"red\",\n",
    "        \"Momentum\": \"blue\",\n",
    "        \"Nesterov\": \"green\",\n",
    "        \"AdaGrad\": \"orange\",\n",
    "        \"RMSProp\": \"purple\",\n",
    "        \"Adam\": \"black\",\n",
    "    }\n",
    "\n",
    "    # Plotten des Pfades anhand der positions-Liste\n",
    "    for idx, (f, positions, z_values, name) in enumerate(optimizer_results):\n",
    "        positions_array = np.array(positions)\n",
    "        color = color_map.get(name, \"gray\")  # Default to gray if name not found\n",
    "\n",
    "        # 3D Pfad\n",
    "        ax1.scatter(\n",
    "            positions_array[:, 0],\n",
    "            positions_array[:, 1],\n",
    "            z_values,\n",
    "            color=color,\n",
    "            s=50,\n",
    "            label=name,\n",
    "        )\n",
    "\n",
    "        # 2D Pfad\n",
    "        ax2.plot(\n",
    "            positions_array[:, 0],\n",
    "            positions_array[:, 1],\n",
    "            marker=\"o\",\n",
    "            color=color,\n",
    "            markersize=5,\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            label=name,\n",
    "        )\n",
    "\n",
    "    # Bereich eingrenzen, um innerhalb von Definitionsbereich zu bleiben\n",
    "    ax1.set_xlim(-20, 20)\n",
    "    ax1.set_ylim(-20, 20)\n",
    "\n",
    "    ax2.set_xlim(-20, 20)\n",
    "    ax2.set_ylim(-20, 20)\n",
    "\n",
    "    # Legende\n",
    "    ax1.legend()\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Funktion, welche Plots um interaktive Elemente ergaenzt\n",
    "def visualize_optimizer(\n",
    "    optimizer_multiselect_options=[\n",
    "        \"Regular\",\n",
    "        \"Momentum\",\n",
    "        \"Nesterov\",\n",
    "        \"AdaGrad\",\n",
    "        \"RMSProp\",\n",
    "        \"Adam\",\n",
    "    ]\n",
    "):\n",
    "    # Optimizer Konfiguration\n",
    "    optimizer_settings = {\n",
    "        \"Regular\": {\n",
    "            \"lr\": widgets.FloatSlider(\n",
    "                value=0.1, min=0, max=2, step=0.01, description=\"Learning Rate\"\n",
    "            )\n",
    "        },\n",
    "        \"Momentum\": {\n",
    "            \"lr\": widgets.FloatSlider(\n",
    "                value=0.1, min=0, max=2, step=0.01, description=\"Learning Rate\"\n",
    "            ),\n",
    "            \"beta1\": widgets.FloatSlider(\n",
    "                value=0.9, min=0, max=1, step=0.01, description=\"Beta1 Factor\"\n",
    "            ),\n",
    "        },\n",
    "        \"Nesterov\": {\n",
    "            \"lr\": widgets.FloatSlider(\n",
    "                value=0.1, min=0, max=2.0, step=0.01, description=\"Learning Rate\"\n",
    "            ),\n",
    "            \"beta1\": widgets.FloatSlider(\n",
    "                value=0.9, min=0, max=1.0, step=0.01, description=\"Beta1 Factor\"\n",
    "            ),\n",
    "        },\n",
    "        \"AdaGrad\": {\n",
    "            \"lr\": widgets.FloatSlider(\n",
    "                value=2.0, min=0, max=2.0, step=0.01, description=\"Learning Rate\"\n",
    "            )\n",
    "        },\n",
    "        \"RMSProp\": {\n",
    "            \"lr\": widgets.FloatSlider(\n",
    "                value=2.0, min=0, max=2, step=0.01, description=\"Learning Rate\"\n",
    "            ),\n",
    "            \"beta1\": widgets.FloatSlider(\n",
    "                value=0.9, min=0, max=1, step=0.01, description=\"Beta1 Factor\"\n",
    "            ),\n",
    "        },\n",
    "        \"Adam\": {\n",
    "            \"lr\": widgets.FloatSlider(\n",
    "                value=2.0, min=0, max=2.0, step=0.01, description=\"Learning Rate\"\n",
    "            ),\n",
    "            \"beta1\": widgets.FloatSlider(\n",
    "                value=0.9, min=0, max=1.0, step=0.01, description=\"Beta1 Factor\"\n",
    "            ),\n",
    "            \"beta2\": widgets.FloatSlider(\n",
    "                value=0.9, min=0, max=1.0, step=0.001, description=\"Beta2 Factor\"\n",
    "            ),\n",
    "        },\n",
    "        \"Custom\": {\n",
    "            \"lr\": widgets.FloatSlider(\n",
    "                value=0.1, min=0, max=2, step=0.01, description=\"Learning Rate\"\n",
    "            ),\n",
    "            \"beta1\": widgets.FloatSlider(\n",
    "                value=0.9, min=0, max=1, step=0.01, description=\"Beta1 Factor\"\n",
    "            ),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Nur ausgewaehlte Optimizer zur Konfiguration freigeben\n",
    "    optimizer_settings = {\n",
    "        key: value\n",
    "        for key, value in optimizer_settings.items()\n",
    "        if key in optimizer_multiselect_options\n",
    "    }\n",
    "\n",
    "    # Funktions-Auswahl\n",
    "    func_dropdown = widgets.Dropdown(\n",
    "        description=\"Function\",\n",
    "        options=[\n",
    "            (\"Steep Valley Function\", steep_valley_function),\n",
    "            (\"Symmetric Convex Function\", symmetric_convex_function),\n",
    "            (\"Asymmetric Convex Function\", asymmetric_convex_function),\n",
    "            (\"L-shaped Valley Function\", l_shaped_valley_function),\n",
    "        ],\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "    x_slider = widgets.IntSlider(\n",
    "        value=15,\n",
    "        min=-20,\n",
    "        max=20,\n",
    "        step=1,\n",
    "        description=\"Starting X\",\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "    y_slider = widgets.IntSlider(\n",
    "        value=10,\n",
    "        min=-20,\n",
    "        max=20,\n",
    "        step=1,\n",
    "        description=\"Starting Y\",\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "    steps_slider = widgets.IntSlider(\n",
    "        value=10,\n",
    "        min=1,\n",
    "        max=30,\n",
    "        step=1,\n",
    "        description=\"Steps\",\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "\n",
    "    # Optimizer-Auswahl\n",
    "    optimizer_multiselect = widgets.SelectMultiple(\n",
    "        description=\"Optimizers\",\n",
    "        options=list(optimizer_settings.keys()),\n",
    "        value=[list(optimizer_settings.keys())[0]],\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "\n",
    "    # Toggle fuer Konfiguration einzelnder Optimizer\n",
    "    optimizer_toggle = widgets.ToggleButtons(\n",
    "        description=\"Configure Optimizer\",\n",
    "        options=list(optimizer_settings.keys()),\n",
    "        value=list(optimizer_settings.keys())[0],\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "\n",
    "    # UI-Platzhalter fuer Konfiguration\n",
    "    optimizer_settings_box = widgets.VBox()\n",
    "\n",
    "    # Optimizer Konfiguration entsprechend Auswahl in UI einbinden\n",
    "    def update_optimizer_settings(change):\n",
    "        selected_optimizer = change[\"new\"]\n",
    "        boxes = [widgets.Label(value=selected_optimizer)]\n",
    "        for setting, widget in optimizer_settings[selected_optimizer].items():\n",
    "            boxes.append(widget)\n",
    "        optimizer_settings_box.children = boxes\n",
    "\n",
    "    optimizer_toggle.observe(update_optimizer_settings, names=\"value\")\n",
    "\n",
    "    # Standard-Auswahl bei Initialisierung\n",
    "    update_optimizer_settings({\"new\": optimizer_toggle.value})\n",
    "\n",
    "    # Learning Rate Scheduler Auswahl\n",
    "    lr_schedulers = [\n",
    "        \"None\",\n",
    "        \"Power\",\n",
    "        \"Exponential\",\n",
    "        \"Piecewise\",\n",
    "        \"Performance\",\n",
    "        \"OneCycle\",\n",
    "    ]\n",
    "    lr_scheduler_dropdown = widgets.Dropdown(\n",
    "        description=\"LR Scheduler\",\n",
    "        options=lr_schedulers,\n",
    "        value=\"None\",\n",
    "        layout=widgets.Layout(width=\"100%\"),\n",
    "    )\n",
    "\n",
    "    # Aktualisieren der Plots\n",
    "    def update_plot(\n",
    "        optimizers, function, steps, initialX, initialY, lr_scheduler, **kwargs\n",
    "    ):\n",
    "        cur_pos = np.array([initialX, initialY])\n",
    "        results = []\n",
    "        for optimizer in optimizers:\n",
    "            params = {\n",
    "                key: kwargs.get(f\"{optimizer}_{key}\")\n",
    "                for key in optimizer_settings[optimizer].keys()\n",
    "            }\n",
    "            scheduler_params = {\n",
    "                \"Power\": 2,\n",
    "                \"Piecewise Boundaries\": \"10,20\",\n",
    "                \"Piecewise Values\": \"0.1,0.01,0.001\",\n",
    "                \"Lambda Factor\": 0.1,\n",
    "                \"Total Steps\": 12,\n",
    "                \"Max LR\": 0.2,\n",
    "            }\n",
    "            results.append(\n",
    "                optimize(\n",
    "                    optimizer,\n",
    "                    function,\n",
    "                    steps,\n",
    "                    cur_pos=cur_pos,\n",
    "                    lr_scheduler=lr_scheduler,\n",
    "                    scheduler_params=scheduler_params,\n",
    "                    **params,\n",
    "                )\n",
    "            )\n",
    "        plot_optimization_path(results)\n",
    "\n",
    "    # Verknuepfen von UI zu Plots\n",
    "    out = widgets.interactive_output(\n",
    "        update_plot,\n",
    "        {\n",
    "            \"optimizers\": optimizer_multiselect,\n",
    "            \"function\": func_dropdown,\n",
    "            \"steps\": steps_slider,\n",
    "            \"initialX\": x_slider,\n",
    "            \"initialY\": y_slider,\n",
    "            \"lr_scheduler\": lr_scheduler_dropdown,\n",
    "            **{\n",
    "                f\"{opt}_{param}\": widget\n",
    "                for opt, settings in optimizer_settings.items()\n",
    "                for param, widget in settings.items()\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # UI-Elemente\n",
    "    ui = widgets.VBox(\n",
    "        [\n",
    "            widgets.HBox([x_slider, y_slider]),\n",
    "            widgets.HBox([steps_slider, func_dropdown]),\n",
    "            optimizer_multiselect,\n",
    "            optimizer_toggle,\n",
    "            optimizer_settings_box,\n",
    "            lr_scheduler_dropdown,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abschnitt 1: Regular vs Momentum Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Hier werden die Functions für den Step und Gradient Descent der zwei ersten Optimizer gesetzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Regular Gradient Descent Step\n",
    "\n",
    "\n",
    "def regular_gradient_step(pos, lr, grad):\n",
    "    return pos - lr * grad(*pos)\n",
    "\n",
    "\n",
    "def regular_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=1,\n",
    "    lr=0.1,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "    lr_scheduler=None,\n",
    "    scheduler_params=None,\n",
    ") -> Tuple[Callable[[float, float], float], List[np.ndarray], List[float], str]:\n",
    "    X, Y, Z, f, grad = function()\n",
    "    positions = [cur_pos]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    for i in range(steps):\n",
    "        lr = apply_lr_scheduler(lr_scheduler, i, lr, scheduler_params)\n",
    "        cur_pos = regular_gradient_step(cur_pos, lr, grad)\n",
    "        cur_pos = np.clip(cur_pos, -20, 20)  # Um Punkte außerhalb von Plot zu vermeiden\n",
    "        positions.append(cur_pos)\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X, Y = {cur_pos}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"Regular\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Momentum Gradient Descent Step\n",
    "\n",
    "\n",
    "def momentum_step(pos, lr, beta, momentum, grad):\n",
    "    gradient = grad(*pos)\n",
    "    momentum = beta * momentum - lr * gradient\n",
    "    pos = pos + momentum\n",
    "\n",
    "    return momentum, pos\n",
    "\n",
    "\n",
    "def momentum_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=1,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "    lr_scheduler=None,\n",
    "    scheduler_params=None,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    momentum = np.zeros(2)\n",
    "    positions = [cur_pos]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    for i in range(steps):\n",
    "        lr = apply_lr_scheduler(lr_scheduler, i, lr, scheduler_params)\n",
    "        momentum, cur_pos = momentum_step(cur_pos, lr, beta1, momentum, grad)\n",
    "        cur_pos = np.clip(cur_pos, -20, 20)  # Um Punkte außerhalb von Plot zu vermeiden\n",
    "        positions.append(cur_pos)\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X, Y = {cur_pos}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"Momentum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task / Visualize\n",
    "Die Aufgabe in Abschnitt 1 besteht darin, mit den Optimierern Regular und Momentum herumzuspielen und sich mit dem Visualisierungstool vertraut zu machen.\n",
    "\n",
    "Versuchen Sie, diese Fragen (für sich selbst) zu beantworten:\n",
    "- Wie verhalten sich die Optimierer bei unterschiedlichen ``Starting X`` und ``Starting Y`` Werten?\n",
    "- Wie verändert sich das Verhalten des Optimierers, wenn die ``Learning Rate`` angepasst wird?\n",
    "- Welcher Teil des Verhaltens des Optimierers wird durch den ``Beta1-Faktor`` beeinflusst?\n",
    "- Wie verhält sich der Optimierer bei verschiedenen ``Function(s)``?\n",
    "\n",
    "Tipps:\n",
    "- Sie können mehrere Optimierer auf einmal auswählen, indem Sie die Umschalttaste während der Auswahl gedrückt halten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830f742449134cbdabe7a3ffbf47f09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='100%'), max=2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3857cf1347c47e4b440c418af1ea677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_optimizer(optimizer_multiselect_options=[\"Regular\", \"Momentum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abschnitt 2: Nesterov Accelerated Gradient (NAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Setup)\n",
    "Dieser Abschnitt bedarf keines weiteren Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Ihre Aufgabe ist es, die Schrittfunktion für den Nesterov Accelerated Gradient Descent (kurz: NAG) zu definieren.\n",
    "\n",
    "Tipps:\n",
    "- Schauen Sie sich die mathematische Definition genau an\n",
    "- Orientieren Sie sich an der Schrittfunktion des Momentum-Optimierers\n",
    "\n",
    "**Formel**\n",
    "\n",
    "*Nesterov:*\n",
    "$$ m \\leftarrow \\beta m - \\eta \\nabla_{\\theta} f(\\theta + \\beta m) $$\n",
    "$$ \\theta \\leftarrow \\theta + m $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesterov_step(pos, lr, beta, momentum, grad):\n",
    "    # TODO implementieren Sie hier die Step-Funktion des Nesterov Accelerated Gradient Descent\n",
    "    # berücksichtigen Sie die Tipps aus der Aufgabenstellung\n",
    "    return pos, momentum\n",
    "\n",
    "\n",
    "def nesterov_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=1,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "    lr_scheduler=None,\n",
    "    scheduler_params=None,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    momentum = np.zeros(2)  # Momentum bei 0 initialisiern\n",
    "    positions = [cur_pos]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    for i in range(steps):\n",
    "        lr = apply_lr_scheduler(lr_scheduler, i, lr, scheduler_params)\n",
    "        cur_pos, momentum = nesterov_step(cur_pos, lr, beta1, momentum, grad)\n",
    "        cur_pos = np.clip(cur_pos, -20, 20)  # Um Punkte außerhalb von Plot zu vermeiden\n",
    "        positions.append(cur_pos.copy())  # Speichere neue Position\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X, Y = {cur_pos}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"Nesterov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "Benutzen Sie diesen Block, um Ihre ergebnisse zu Visualisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d7092e40cb46d69d91620c3b6cb0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='100%'), max=2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343008e97f334d5384f1bcdd9b59a9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_optimizer(optimizer_multiselect_options=[\"Nesterov\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abschnitt 3: AdaGrad, RMSProp und Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Hier werden die Functions für den Step und Gradient Descent der Optimizer Adagrad, RMSProp und Adam gesetzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "adagrad_s = []\n",
    "rmsprop_s = []\n",
    "\n",
    "\n",
    "def adagrad_step(pos, lr, grad, s, epsilon=1e-8):\n",
    "    gradient = grad(*pos)\n",
    "    s = s + gradient**2\n",
    "    pos = pos - (lr * gradient / (np.sqrt(s + epsilon)))\n",
    "    return pos, s\n",
    "\n",
    "\n",
    "def adagrad_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "    lr_scheduler=None,\n",
    "    scheduler_params=None,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    positions = [cur_pos.copy()]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "    global adagrad_s\n",
    "    s = np.zeros(2)  # Initiale Summe der Gradienten\n",
    "    adagrad_s = []\n",
    "    for i in range(steps):\n",
    "        lr = apply_lr_scheduler(lr_scheduler, i, lr, scheduler_params)\n",
    "        cur_pos, s = adagrad_step(cur_pos, lr, grad, s)\n",
    "        adagrad_s.append(s.copy())\n",
    "        cur_pos = np.clip(cur_pos, -20, 20)  # Um Punkte außerhalb von Plot zu vermeiden\n",
    "        positions.append(cur_pos.copy())  # Speichere aktuelle Position\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"AdaGrad\")\n",
    "\n",
    "\n",
    "def rmsprop_step(pos, lr, grad, s, beta=0.9, epsilon=1e-8):\n",
    "    gradient = grad(*pos)\n",
    "    s = beta * s + (1 - beta) * gradient**2\n",
    "    pos = pos - (lr * gradient / (np.sqrt(s + epsilon)))\n",
    "    return pos, s\n",
    "\n",
    "\n",
    "def rmsprop_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "    lr_scheduler=None,\n",
    "    scheduler_params=None,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    positions = [cur_pos.copy()]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "    global rmsprop_s\n",
    "    s = np.zeros_like(cur_pos)\n",
    "    rmsprop_s = []\n",
    "    for i in range(steps):\n",
    "        lr = apply_lr_scheduler(lr_scheduler, i, lr, scheduler_params)\n",
    "        cur_pos, s = rmsprop_step(cur_pos, lr, grad, s, beta1)  # RMS Step\n",
    "        rmsprop_s.append(s.copy())\n",
    "        cur_pos = np.clip(cur_pos, -20, 20)  # Um Punkte außerhalb von Plot zu vermeiden\n",
    "        positions.append(cur_pos.copy())  # Speichere aktuelle Position\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"RMSProp\")\n",
    "\n",
    "\n",
    "def adam_step(pos, lr, grad, m, v, t, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    gradient = grad(*pos)\n",
    "    m = (\n",
    "        beta1 * m + (1 - beta1) * gradient\n",
    "    )  # Aktualisierung der verzerrten Schaetzung des ersten Moments\n",
    "    v = (\n",
    "        beta2 * v + (1 - beta2) * gradient**2\n",
    "    )  # Aktualisierung der verzerrten Schaetzung des zweiten Moments\n",
    "    m_hat = m / (\n",
    "        1 - beta1**t\n",
    "    )  # Berechnung der verzerrungskorrigierten Schaetzung des ersten Moments\n",
    "    v_hat = v / (\n",
    "        1 - beta2**t\n",
    "    )  # Berechnung der verzerrungskorrigierten Schaetzung des zweiten Moments\n",
    "    pos = pos - lr * m_hat / (np.sqrt(v_hat) + epsilon)  # Aktualisiere Parameter\n",
    "    return pos, m, v\n",
    "\n",
    "\n",
    "def adam_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    beta2=0.99,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "    lr_scheduler=None,\n",
    "    scheduler_params=None,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    positions = [cur_pos.copy()]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    m = np.zeros_like(cur_pos)  # Initialisierung Vektor ersten Moments\n",
    "    v = np.zeros_like(cur_pos)  # Initialisierung Vektor zweiten Moments\n",
    "    t = 0  # Initialisierung timestep\n",
    "\n",
    "    positions = [cur_pos.copy()]  # Speichere Start-Position\n",
    "\n",
    "    for i in range(1, steps + 1):\n",
    "        lr = apply_lr_scheduler(lr_scheduler, i, lr, scheduler_params)\n",
    "        t += 1  # Erhoehen timestep\n",
    "        cur_pos, m, v = adam_step(cur_pos, lr, grad, m, v, t, beta1, beta2)  # Adam Step\n",
    "        cur_pos = np.clip(cur_pos, -20, 20)  # Um Punkte außerhalb von Plot zu vermeiden\n",
    "        positions.append(cur_pos.copy())  # Speichere aktuelle Position\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"Adam\")\n",
    "\n",
    "\n",
    "def plot_s_values(adagrad_s, rmsprop_s):\n",
    "    if not np.array(adagrad_s).any() or not np.array(rmsprop_s).any():\n",
    "        raise SystemExit(\"Verwende erste beide Optimizer in der Visualisierung (Mehrfachauswahl durch Shift + Click möglich)\")\n",
    "\n",
    "    # Konvertierte Listen\n",
    "    adagrad_s = np.array(adagrad_s)\n",
    "    rmsprop_s = np.array(rmsprop_s)\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # Plot fuer jede Dimenstion\n",
    "    plt.plot(adagrad_s[:, 0], label=\"AdaGrad - X\", marker=\"o\", color=\"orange\")\n",
    "    plt.plot(\n",
    "        adagrad_s[:, 1],\n",
    "        label=\"AdaGrad - Y\",\n",
    "        marker=\"o\",\n",
    "        linestyle=\"dashed\",\n",
    "        color=\"orange\",\n",
    "    )\n",
    "    plt.plot(rmsprop_s[:, 0], label=\"RMSProp - X\", marker=\"x\", color=\"purple\")\n",
    "    plt.plot(\n",
    "        rmsprop_s[:, 1],\n",
    "        label=\"RMSProp - Y\",\n",
    "        marker=\"x\",\n",
    "        linestyle=\"dashed\",\n",
    "        color=\"purple\",\n",
    "    )\n",
    "\n",
    "    plt.title(\"Evolution of s over Steps for AdaGrad and RMSProp\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"s\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Ihre Aufgabe ist es, den Verlauf der Variable $s$ sowohl für Adagrad als auch für RMSProp zu erklären.\n",
    "\n",
    "Beantworten Sie dafür die folgenden Fragen:\n",
    "- Welcher Unterschied ist bei der Entwicklung von $s$ zwischen den Optimizern erkennbar? \n",
    "- Warum ist das der Fall?\n",
    "- Welche Rolle spielt $s$ in beiden Optimierern?\n",
    "- Wie reagiert $s$ auf Änderungen der Parameters?\n",
    "\n",
    "**Formeln:**\n",
    "\n",
    "*AdaGrad:*\n",
    "$$ \\mathbf{s} \\leftarrow \\mathbf{s} + \\nabla_{\\theta} f(\\theta) \\otimes \\nabla_{\\theta} f(\\theta) $$\n",
    "$$ \\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} f(\\theta) \\oslash \\sqrt{\\mathbf{s} + \\epsilon} $$\n",
    "\n",
    "*RMSProp:*\n",
    "$$ \\mathbf{s} \\leftarrow \\beta \\mathbf{s} + (1-\\beta) \\nabla_{\\theta} f(\\theta) \\otimes \\nabla_{\\theta} f(\\theta) $$\n",
    "$$ \\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} f(\\theta) \\oslash \\sqrt{\\mathbf{s} + \\epsilon} $$\n",
    "\n",
    "### Visualize\n",
    "Verwenden Sie die Visualisierung der Optimizer, sowie der s_values, um die Aufgabe zu bearbeiten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bfc9413c8444e1cbd7e05dea8fc8c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='100%'), max=2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59bf06611a094194b94e1895ceef7ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_optimizer(optimizer_multiselect_options=[\"AdaGrad\", \"RMSProp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Verwende erste beide Optimizer in der Visualisierung (Mehrfachauswahl durch Shift + Click möglich)",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m Verwende erste beide Optimizer in der Visualisierung (Mehrfachauswahl durch Shift + Click möglich)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3405: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "plot_s_values(adagrad_s, rmsprop_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abschnitt X: Eigener Optimizer\n",
    "### Task\n",
    "In diesem Abschnitt sollen Sie ihren eigenen Optimizer implementieren mit dem Haken, dass Zufall eingebunden werden muss.\n",
    "\n",
    "Ziel ist es einen kreativen Optimizer zu entwickeln, der zum Minimum hinkonvergiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb15f1dc2e604fceaede09664c5da4b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=15, description='Starting X', layout=Layout(width='100%'), max=2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd0ddf4d3d541b0a9910069c20c5bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def custom_step(pos, lr, grad):\n",
    "    pos = pos - lr * grad(*pos)\n",
    "    # TODO Implementiere eigene Logik\n",
    "    # Zufall kann durch randint(a,b) eingebunden werden\n",
    "    return pos\n",
    "\n",
    "\n",
    "def custom_gradient_descent(\n",
    "    function=steep_valley_function,\n",
    "    steps=10,\n",
    "    lr=0.1,\n",
    "    beta1=0.9,\n",
    "    cur_pos=np.array([15, 18]),\n",
    "    print_res=False,\n",
    "    lr_scheduler=None,\n",
    "    scheduler_params=None,\n",
    "):\n",
    "    X, Y, Z, f, grad = function()\n",
    "    positions = [cur_pos.copy()]\n",
    "    z_values = [f(cur_pos[0], cur_pos[1])]\n",
    "\n",
    "    positions = [cur_pos.copy()]  # Speichern der Start-Positionen\n",
    "\n",
    "    for i in range(1, steps + 1):\n",
    "        lr = apply_lr_scheduler(lr_scheduler, i, lr, scheduler_params)\n",
    "        cur_pos = custom_step(cur_pos, lr, grad)  # Custom Step\n",
    "        cur_pos = np.clip(\n",
    "            cur_pos, -20, 20\n",
    "        )  # Um Punkte außerhalb des Plots zu vermeiden\n",
    "        positions.append(cur_pos.copy())  # Speichere aktuelle Position\n",
    "        cur_z = f(cur_pos[0], cur_pos[1])\n",
    "        z_values.append(cur_z)\n",
    "        if print_res:\n",
    "            print(f\"Step {i}: X = {cur_pos[0]}, Y = {cur_pos[1]}, Z = {cur_z}\")\n",
    "\n",
    "    return (f, positions, z_values, \"Custom\")\n",
    "\n",
    "\n",
    "visualize_optimizer([\"Custom\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_lars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
